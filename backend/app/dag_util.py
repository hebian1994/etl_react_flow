from DB import DATABASE_FILE
import sqlite3
import json
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt

# ========== åç«¯åŸºç±»æ¥å£ ==========


class ETLBackend:
    def read_csv(self, params): raise NotImplementedError
    def filter(self, df, params): raise NotImplementedError
    def left_join(self, df1, df2, params): raise NotImplementedError
    def aggregate(self, df, params): raise NotImplementedError
    def output(self, df, params): return df  # é»˜è®¤ç›´æ¥è¿”å›

# ========== Pandas å®ç° ==========


class PandasBackend(ETLBackend):
    def read_csv(self, _, params):
        print(params)
        print(_)
        # è·å–æ–‡ä»¶ç¬¬ä¸€è¡Œå’Œç¬¬äºŒè¡Œçš„å†…å®¹ï¼Œç„¶åæ ¹æ®é€—å·åˆ‡å‰²å¾—åˆ°å­—æ®µåå’Œæ•°æ®ç±»å‹,æ³¨æ„å»æ‰ç»“å°¾çš„æ¢è¡Œç¬¦
        with open(params["path"].strip('"'), 'r', encoding='utf-8') as f:
            first_line = f.readline().strip()
            second_line = f.readline().strip()
        print(f"first_line: {first_line}")
        print(f"second_line: {second_line}")
        # æ ¹æ®é€—å·åˆ‡å‰²å¾—åˆ°å­—æ®µåå’Œæ•°æ®ç±»å‹
        field_names = first_line.split(',')
        field_types = second_line.split(',')

        # æ–‡ä»¶çš„ç¬¬ä¸€è¡Œä¸ºè¡¨å¤´ï¼Œç¬¬äºŒè¡Œä¸ºæ•°æ®ç±»å‹ï¼Œç¬¬ä¸‰è¡Œå¼€å§‹ä¸ºæ•°æ®
        # éœ€è¦æ ¹æ®ç¬¬äºŒè¡Œçš„æŒ‡å®šæ•°æ®ç±»å‹è¯»å–
        # éœ€è¦æŒ‡å®šåˆ—å
        df = pd.read_csv(params["path"].strip(
            '"'), header=2, dtype=dict(zip(field_names, field_types)), names=field_names)

        return df

    def filter(self, df, params):
        print(f"filter params: {params}")
        if params["condition"] == "":
            print("condition is empty")
            return df
        else:
            return df.query(params["condition"])

    def left_join(self, df1, df2, params):
        print(f"left_join params: {params}")
        if 'left_join_on' not in params:
            print("left_join_on is empty")
            return df1
        else:
            print(f"left_join_on: {params['left_join_on']}")
            # will get join condition as "name=name and age=age"
            # will get join condition as "name=name and age=age"
            join_condition = params["left_join_on"].split(" and ")
            # can use left_on and right_on to specify the join condition
            left_on_arr = []
            right_on_arr = []
            for condition in join_condition:
                left_on, right_on = condition.split("=")
                left_on_arr.append(left_on)
                right_on_arr.append(right_on)
            df1 = df1.merge(df2, left_on=left_on_arr,
                            right_on=right_on_arr, how="left")

            df1 = df1.fillna("")

            return df1

    def aggregate(self, df, params):
        return df.groupby(params["by"]).agg({"age": "mean"}).reset_index(drop=False)

# ========== Polars ç¤ºä¾‹ï¼ˆå¯é€‰ï¼‰ ==========
# import polars as pl
# class PolarsBackend(ETLBackend):
#     def read_csv(self, params):
#         return pl.read_csv(params["path"])
#     def filter(self, df, params):
#         return df.filter(pl.col(params["col"]) > params["value"])
#     def aggregate(self, df, params):
#         return df.groupby(params["by"]).agg(pl.col("age").mean())

# ========== æ“ä½œè°ƒåº¦å™¨ ==========


class ETLOperator:
    def __init__(self, backend: ETLBackend):
        self.backend = backend
        self.dispatch = {
            "read_csv": self.backend.read_csv,
            "File Input": self.backend.read_csv,
            "Filter": self.backend.filter,
            "Left Join": self.backend.left_join,
            "aggregate": self.backend.aggregate,
            "output": self.backend.output,
            "Data Viewer": self.backend.output,
        }

    def run(self, op_name, input_df, params):
        return self.dispatch[op_name](input_df, params)

    def run_left_join(self, op_name, input_df1, input_df2, params):
        return self.dispatch[op_name](input_df1, input_df2, params)

# ========== DAG æ‰§è¡Œå™¨ ==========


def execute_dag(nodes, edges, backend_name="pandas"):
    if backend_name == "pandas":
        backend = PandasBackend()
    # elif backend_name == "polars":
    #     backend = PolarsBackend()
    else:
        raise ValueError(f"Unsupported backend: {backend_name}")

    operator = ETLOperator(backend)

    G = nx.DiGraph()
    for node in nodes:
        G.add_node(node["id"], op=node["type"], params=node["params"])
    for edge in edges:
        G.add_edge(edge["source"], edge["target"])

    # draw_dag(G)

    sorted_nodes = list(nx.topological_sort(G))
    results = {}

    for node_id in sorted_nodes:
        op = G.nodes[node_id]["op"]
        params = G.nodes[node_id]["params"]
        preds = list(G.predecessors(node_id))
        inputs = [results[p] for p in preds] if preds else [None]

        print(f"inputs: {inputs}")
        # å¯èƒ½ä¸æ˜¯åªæœ‰inputs[0]ï¼Œè€Œæ˜¯inputs[0]å’Œinputs[1]
        if len(inputs) == 2:
            # handle left join
            result = operator.run_left_join(op, inputs[0], inputs[1], params)
        else:
            result = operator.run(op, inputs[0], params)
        results[node_id] = result

    return results

# ========== å¯è§†åŒ– ==========


def draw_dag(G):
    plt.figure(figsize=(8, 6))
    pos = nx.spring_layout(G)
    labels = {n: f"{n}: {G.nodes[n]['op']}" for n in G.nodes}
    nx.draw(G, pos, with_labels=True, labels=labels, node_color='lightgreen',
            node_size=2000, font_size=10, arrows=True, arrowsize=20)
    plt.title("ETL æ‰§è¡Œ DAG å›¾")
    plt.tight_layout()
    plt.show()


def build_flowchart_data(flow_id, flows_file):
    # åŠ è½½ flows.txtï¼ˆä»ç„¶ä¿ç•™ä¸ºæ–‡ä»¶ï¼Œå› ä¸ºæ²¡æœ‰è¿ç§»åˆ° DBï¼‰
    print(f"flow_id: {flow_id}")
    with open(flows_file, 'r', encoding='utf-8') as f:
        flows = [json.loads(line) for line in f if line.strip()]

    print(f"flows: {flows}")

    for f in flows:
        print(f"f: {f}")
        print(f"f['flow_id']: {f['flow_id']}")

    target_flow = next((f for f in flows if f["flow_id"] == flow_id), None)
    if not target_flow:
        raise ValueError(f"Flow ID {flow_id} not found.")

    # ä» SQLite ä¸­åŠ è½½ nodes è¡¨æ•°æ®ï¼ˆnode_type_mapï¼‰
    node_type_map = {}
    conn = sqlite3.connect(DATABASE_FILE)
    c = conn.cursor()

    c.execute("SELECT id, type FROM nodes")
    for row in c.fetchall():
        node_id, node_type = row
        node_type_map[node_id] = node_type

    # ä» SQLite ä¸­åŠ è½½ node_configs è¡¨æ•°æ®ï¼ˆnode_config_mapï¼‰
    node_config_map = {}
    c.execute(
        "SELECT node_id, config_name, config_param FROM node_configs")
    for row in c.fetchall():
        node_id, config_name, config_param = row
        print(
            f"node_id: {node_id}, config_name: {config_name}, config_param: {config_param}")
        if node_id not in node_config_map:
            node_config_map[node_id] = []
        print(f'will append {config_name}: {config_param} to {node_id}')
        node_config_map[node_id].append({config_name: config_param})

    conn.close()

    print("node_config_map", node_config_map)

    # æ„å»ºèŠ‚ç‚¹
    nodes = []
    for node in target_flow['nodes']:
        node_id = node['id']
        node_type = node_type_map.get(node_id, "unknown")
        params = {}
        if node_type == "File Input":
            config_param_arr = node_config_map.get(node_id, [])
            for config_param_dict in config_param_arr:
                params = {"path": config_param_dict['path']}
        if node_type == "Filter":
            config_param_arr = node_config_map.get(node_id, [])
            for config_param_dict in config_param_arr:
                params = {"condition": config_param_dict['condition']}
        if node_type == "Left Join":
            config_param_arr = node_config_map.get(node_id, [])
            for config_param_dict in config_param_arr:
                params = {"left_join_on": config_param_dict['left_join_on']}
        nodes.append({
            "id": node_id,
            "type": node_type,
            "params": params
        })

    print("nodes", nodes)

    # æ„å»ºè¾¹
    edges = []
    for edge in target_flow['edges']:
        edges.append({
            "source": edge['source'],
            "target": edge['target']
        })

    return {"nodes": nodes, "edges": edges}


# flowchart_data = build_flowchart_data(
#     flow_id="43cd13c7-25b3-42f3-8d89-6ea353ac5daa",
#     flows_file="flows.txt"
# )
# print(flowchart_data)

# ========== æµ‹è¯•æ•°æ®ç»“æ„ ==========
flowchart_data = build_flowchart_data(
    flow_id='2789aec8-d8c0-40c5-bbcb-1023d15a81c1',
    flows_file="flows.txt"
)

res = execute_dag(
    nodes=flowchart_data["nodes"],
    edges=flowchart_data["edges"],
    backend_name="pandas"  # ğŸ‘ˆ å¯ä»¥åˆ‡æ¢ä¸º "polars"
)

# print("æœ€ç»ˆç»“æœï¼š")
print(res)
